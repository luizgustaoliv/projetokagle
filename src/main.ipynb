{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad1f1bdc",
   "metadata": {},
   "source": [
    "## Importação de Bibliotecas e Configuração Inicial\n",
    "\n",
    "Esta célula realiza a importação de todas as bibliotecas necessárias para o pipeline de machine learning e define os caminhos dos arquivos:\n",
    "\n",
    "### Bibliotecas Importadas:\n",
    "\n",
    "**Utilitários Gerais:**\n",
    "- `numpy`: Operações numéricas e arrays\n",
    "- `pandas`: Manipulação de dataframes\n",
    "\n",
    "**Otimização e Paralelização:**\n",
    "- `itertools.product`: Geração de combinações de hiperparâmetros\n",
    "- `joblib.Parallel, delayed`: Processamento paralelo para otimização\n",
    "\n",
    "**Pré-processamento (sklearn):**\n",
    "- `SimpleImputer`: Tratamento de valores ausentes\n",
    "- `StandardScaler`: Normalização de features numéricas\n",
    "- `OneHotEncoder`: Codificação de variáveis categóricas\n",
    "- `ColumnTransformer`: Aplicação de transformações específicas por tipo de coluna\n",
    "- `Pipeline`: Encadeamento de etapas de processamento\n",
    "\n",
    "**Modelo e Validação:**\n",
    "- `HistGradientBoostingClassifier`: Modelo de gradient boosting otimizado\n",
    "- `StratifiedKFold`: Validação cruzada estratificada\n",
    "- `accuracy_score`: Métrica de avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "TRAIN_PATH = 'train.csv'\n",
    "TEST_PATH = 'test.csv'\n",
    "SAMPLE_PATH = 'sample_submission.csv'\n",
    "OUTPUT_PATH = 'submission.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46536453",
   "metadata": {},
   "source": [
    "## Funções Auxiliares do Pipeline\n",
    "\n",
    "### `load_data()`\n",
    "Carrega os arquivos CSV (train, test, sample) e valida sua existência. Retorna os 3 DataFrames.\n",
    "\n",
    "### `basic_feature_engineering(df)`\n",
    "Cria features derivadas:\n",
    "- Diferenças temporais (`age_first_last_diff`)\n",
    "- Transformações log (`log_funding_total`)\n",
    "- Razões (`funding_per_round`)\n",
    "- Flags binárias (`has_milestones`, `missing_*`)\n",
    "- Interações entre features (`funding_milestones_interaction`)\n",
    "\n",
    "### `get_feature_lists(X)`\n",
    "Separa automaticamente colunas numéricas e categóricas do dataset.\n",
    "\n",
    "### `build_preprocessor(numeric_cols, cat_cols)`\n",
    "Cria pipeline sklearn:\n",
    "- **Numéricas:** Imputação (mediana) → Normalização (StandardScaler)\n",
    "- **Categóricas:** Imputação (moda) → One-Hot Encoding\n",
    "\n",
    "### `optimize_threshold_acc(y_true, y_probs)`\n",
    "\n",
    "Encontra o threshold ótimo que maximiza a acurácia ao converter probabilidades em predições binárias.\n",
    "\n",
    "**Funcionamento:**\n",
    "- Testa 101 thresholds de 0.0 a 1.0 (incrementos de 0.01)\n",
    "- Para cada threshold, converte probabilidades em classes (0 ou 1)\n",
    "- Calcula a acurácia e mantém o threshold com melhor resultado\n",
    "- **Retorna:** threshold ótimo (padrão: 0.5 se nenhum melhorar)\n",
    "\n",
    "**Uso:** Permite ajustar o ponto de corte de classificação além do padrão 0.5, útil para datasets desbalanceados ou quando se busca maximizar acurácia específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b7207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Funções\n",
    "# -----------------------------\n",
    "def load_data():\n",
    "    for p in [TRAIN_PATH, TEST_PATH, SAMPLE_PATH]:\n",
    "        if not os.path.exists(p):\n",
    "            raise FileNotFoundError(f\"Required file not found: {p}\")\n",
    "    train = pd.read_csv(TRAIN_PATH)\n",
    "    test = pd.read_csv(TEST_PATH)\n",
    "    sample = pd.read_csv(SAMPLE_PATH)\n",
    "    return train, test, sample\n",
    "\n",
    "def basic_feature_engineering(df):\n",
    "    df = df.copy()\n",
    "    if 'age_first_funding_year' in df.columns and 'age_last_funding_year' in df.columns:\n",
    "        df['age_first_last_diff'] = df['age_last_funding_year'].fillna(0) - df['age_first_funding_year'].fillna(0)\n",
    "    if 'funding_total_usd' in df.columns:\n",
    "        df['missing_funding_total'] = df['funding_total_usd'].isna().astype(int)\n",
    "        df['log_funding_total'] = np.log1p(df['funding_total_usd'].fillna(0))\n",
    "    if 'funding_rounds' in df.columns and 'funding_total_usd' in df.columns:\n",
    "        df['funding_per_round'] = df['funding_total_usd'] / (df['funding_rounds'].replace(0, np.nan).fillna(1))\n",
    "    if 'milestones' in df.columns:\n",
    "        df['has_milestones'] = (df['milestones'] > 0).astype(int)\n",
    "    for c in ['age_first_funding_year', 'age_last_funding_year',\n",
    "              'age_first_milestone_year', 'age_last_milestone_year']:\n",
    "        if c in df.columns:\n",
    "            df[f'missing_{c}'] = df[c].isna().astype(int)\n",
    "    if 'funding_per_round' in df.columns and 'has_milestones' in df.columns:\n",
    "        df['funding_milestones_interaction'] = df['funding_per_round'] * df['has_milestones']\n",
    "    if 'log_funding_total' in df.columns and 'age_first_last_diff' in df.columns:\n",
    "        df['log_funding_by_age_diff'] = df['log_funding_total'] / (df['age_first_last_diff'].replace(0,1))\n",
    "    return df\n",
    "\n",
    "def get_feature_lists(X):\n",
    "    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    return numeric_cols, cat_cols\n",
    "\n",
    "def build_preprocessor(numeric_cols, cat_cols):\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_cols),\n",
    "            ('cat', categorical_transformer, cat_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "def optimize_threshold_acc(y_true, y_probs):\n",
    "    best_thresh = 0.5\n",
    "    best_score = -1\n",
    "    for t in np.linspace(0, 1, 101):\n",
    "        preds = (y_probs >= t).astype(int)\n",
    "        score = accuracy_score(y_true, preds)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_thresh = t\n",
    "    return best_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea2077",
   "metadata": {},
   "source": [
    "## Avaliação de Combinação de Hiperparâmetros\n",
    "\n",
    "### `evaluate_combo(params, X, y, preprocessor, skf)`\n",
    "\n",
    "Avalia uma combinação específica de hiperparâmetros usando validação cruzada estratificada.\n",
    "\n",
    "**Parâmetros testados:**\n",
    "- `lr`: Learning rate\n",
    "- `depth`: Profundidade máxima da árvore\n",
    "- `leaves`: Número máximo de folhas\n",
    "- `min_leaf`: Mínimo de amostras por folha\n",
    "- `l2`: Regularização L2\n",
    "\n",
    "**Processo:**\n",
    "1. Cria pipeline com preprocessador + HistGradientBoostingClassifier\n",
    "2. Executa validação cruzada (StratifiedKFold)\n",
    "3. Para cada fold: treina, prediz probabilidades no conjunto de validação\n",
    "4. Acumula predições out-of-fold\n",
    "5. Otimiza threshold nas predições completas\n",
    "6. Calcula acurácia final com threshold otimizado\n",
    "\n",
    "**Retorna:** `(score, params, thresh)` - acurácia, parâmetros testados e melhor threshold\n",
    "\n",
    "**Uso:** Permite busca em grid paralelizável com avaliação robusta via CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2873d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_combo(params, X, y, preprocessor, skf):\n",
    "    lr, depth, leaves, min_leaf, l2 = params\n",
    "    val_preds = np.zeros(len(X))\n",
    "    pipe = Pipeline([\n",
    "        ('pre', preprocessor),\n",
    "        ('clf', HistGradientBoostingClassifier(\n",
    "            learning_rate=lr,\n",
    "            max_iter=1000,\n",
    "            max_depth=depth,\n",
    "            max_leaf_nodes=leaves,\n",
    "            min_samples_leaf=min_leaf,\n",
    "            l2_regularization=l2,\n",
    "            early_stopping=False,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr = y.iloc[train_idx]\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        val_preds[val_idx] = pipe.predict_proba(X_val)[:,1]\n",
    "    thresh = optimize_threshold_acc(y, val_preds)\n",
    "    score = accuracy_score(y, (val_preds >= thresh).astype(int))\n",
    "    return (score, params, thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4803741b",
   "metadata": {},
   "source": [
    "## Função Principal (`main`)\n",
    "\n",
    "### 1. Carregamento e Preparação\n",
    "- Carrega datasets (train, test, sample)\n",
    "- Aplica feature engineering em train e test\n",
    "- Separa features (X) e target (y)\n",
    "- Identifica colunas numéricas e categóricas\n",
    "- Cria preprocessador\n",
    "\n",
    "### 2. Grid Search Paralelizado\n",
    "**Hiperparâmetros testados:**\n",
    "- `learning_rate`: [0.03, 0.05, 0.1]\n",
    "- `max_depth`: [4, 5, 6]\n",
    "- `max_leaf_nodes`: [63, 127]\n",
    "- `min_samples_leaf`: [3, 5, 10]\n",
    "- `l2_regularization`: [0.0, 0.1]\n",
    "\n",
    "**Total:** 108 combinações (3×3×2×3×2)\n",
    "\n",
    "- Gera todas as combinações com `itertools.product`\n",
    "- Avalia paralelamente com `joblib.Parallel` (usa todos os cores)\n",
    "- Cada combo usa validação cruzada 5-fold estratificada\n",
    "- Seleciona melhor combinação por acurácia\n",
    "\n",
    "### 3. Treinamento Final e Predição\n",
    "- Treina modelo com melhores hiperparâmetros no dataset completo\n",
    "- Prediz probabilidades no test set\n",
    "- Aplica threshold otimizado para classificação binária\n",
    "- Gera arquivo `submission.csv`\n",
    "\n",
    "**Saída:** Arquivo de submissão com predições finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05776db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novo melhor F1=0.8412 | params=(0.03, 4, 63, 3, 0.0) | threshold=0.500\n",
      "Novo melhor F1=0.8426 | params=(0.03, 4, 63, 5, 0.0) | threshold=0.500\n",
      "Novo melhor F1=0.8472 | params=(0.03, 4, 63, 5, 0.1) | threshold=0.500\n",
      "Melhor configuração encontrada: (0.03, 4, 63, 5, 0.1) | F1=0.8472\n",
      "Submission salvo em submission.csv | linhas=277 | threshold=0.500\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    train, test, sample = load_data()\n",
    "\n",
    "    # Feature engineering\n",
    "    train_fe = basic_feature_engineering(train)\n",
    "    test_fe = basic_feature_engineering(test)\n",
    "\n",
    "    # Preparação de dados\n",
    "    id_col = 'id' if 'id' in train.columns else None\n",
    "    target_col = 'labels'\n",
    "    drop_cols = [id_col] if id_col else []\n",
    "    X = train_fe.drop(columns=drop_cols + [target_col], errors='ignore')\n",
    "    y = train_fe[target_col].copy()\n",
    "    X_test = test_fe.drop(columns=[id_col] if id_col else [], errors='ignore')\n",
    "\n",
    "    # Preprocessamento\n",
    "    numeric_cols, cat_cols = get_feature_lists(X)\n",
    "    preprocessor = build_preprocessor(numeric_cols, cat_cols)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Grid search completo paralelizado\n",
    "    # -----------------------------\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.03, 0.05, 0.1],\n",
    "        'max_depth': [4,5,6],\n",
    "        'max_leaf_nodes': [63,127],\n",
    "        'min_samples_leaf': [3,5,10],\n",
    "        'l2_regularization': [0.0,0.1]\n",
    "    }\n",
    "\n",
    "    combos = list(product(\n",
    "        param_grid['learning_rate'],\n",
    "        param_grid['max_depth'],\n",
    "        param_grid['max_leaf_nodes'],\n",
    "        param_grid['min_samples_leaf'],\n",
    "        param_grid['l2_regularization']\n",
    "    ))\n",
    "\n",
    "    results = Parallel(n_jobs=-1, verbose=10)(\n",
    "        delayed(evaluate_combo)(params, X, y, preprocessor, skf) for params in combos\n",
    "    )\n",
    "\n",
    "    # Seleciona melhor\n",
    "    best_score, best_params, best_thresh = max(results, key=lambda x: x[0])\n",
    "    print(f\"Melhor configuração encontrada: {best_params} | Accuracy={best_score:.4f} | threshold={best_thresh:.3f}\")\n",
    "\n",
    "    # Treina HGB completo com os melhores hiperparâmetros\n",
    "    lr, depth, leaves, min_leaf, l2 = best_params\n",
    "    pipe_hgb_full = Pipeline([\n",
    "        ('pre', preprocessor),\n",
    "        ('clf', HistGradientBoostingClassifier(\n",
    "            learning_rate=lr,\n",
    "            max_iter=1000,\n",
    "            max_depth=depth,\n",
    "            max_leaf_nodes=leaves,\n",
    "            min_samples_leaf=min_leaf,\n",
    "            l2_regularization=l2,\n",
    "            early_stopping=False,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    pipe_hgb_full.fit(X, y)\n",
    "\n",
    "    # Predições teste\n",
    "    test_proba = pipe_hgb_full.predict_proba(X_test)[:,1]\n",
    "    preds_test = (test_proba >= best_thresh).astype(int)\n",
    "\n",
    "    # Gera submission\n",
    "    submission = sample.copy()\n",
    "    label_col = [c for c in submission.columns if c != 'id'][0]\n",
    "    submission[label_col] = preds_test\n",
    "    submission.to_csv(OUTPUT_PATH, index=False)\n",
    "    print(f\"Submission salvo em {OUTPUT_PATH} | linhas={len(submission)} | threshold={best_thresh:.3f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
